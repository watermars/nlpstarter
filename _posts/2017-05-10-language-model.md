# 语言模型


----------
* 引言
* 马尔科夫模型
 - 定长序列的马尔科夫模型
 - 变长序列的马尔科夫模型
* Trigram 语言模型
 - 基本定义
 - 最大似然估计
 - 语言模型的评价指标：困惑度（Perplexity）
 - Trigram 语言模型的优缺点
* Trigram模型的平滑估计
 - 线性插值法
 - 折损方法

----------
**原标题：** [Chapter 1 Language Modeling]( http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf)
**原作者：** [Michael Collins](http://www.cs.columbia.edu/~mcollins/) Vikram S. Pandit Professor of Computer Science, Columbia University
**译  者：** 佚名


## **1.1 引言**

在本章我们主要讨论如何使用特定语言的一组句子来创建该语言的语言模型的问题。语言模型最初是为了解决语音识别问题而提出的，在现代的语音识别方法中语言模型依然扮演者重要的角色，语言模型也广泛应用与其他NLP应用中。在本章中提到的语言模型参数估计方法也可以使用在其他问题参数估计方法中，例如：标注和解析等问题。
语言模型建模问题的任务是：在特定语言的语料集下，估计该语言模型参数的过程。假设我们有一个语料集（特定语言的句子集合）。例如我们收集了几年的纽约时报正文或者我们从web中抓取了大量的文本数据。我们针对不同的语料集可以估计不同的语言模型参数，进而建立不同的语言模型。
语言模型的定义如下： $\mathcal{V}$ 是特定语言中所有词的集合。例如对于英语我们建立一个英语的语言模型有 $$\mathcal{V} = \{the, dog, laughs, saw, barks, cat \}$$
在实际中 $\mathcal{V}$ 可能非常大，包含成百上千的词语。我们假设 $\mathcal{V}$ 是一个有限集。句子定义为一个语言中的一个词序列
$$x_1x_2\dots x_n$$
其中 $n$ 为整数且 $n \geq 1, x_i \in \mathcal{V}, i \in \{1 \dots (n - 1) \}$, 我们假设 $x_n$ 是一个特殊符号**STOP**（假设STOP属于集合$\mathcal{V}$）。假设每一个句子都是以STOP结尾会给后面的计算带来很多方便。例如下面的句子：
```
the dog barks STOP
the cat laughs STOP
the cat saw the dog STOP
cat the dog the STOP
cat cat cat STOP
STOP
...
```
定义集合$\mathcal{V}^{\dagger}$为在集合$\mathcal{V}$上产生的所有句子的集合。集合$mathcal{V}^{\dagger}$是一个无限集，因为句子的长度可以是任意实数。
现在我们给出如下定义。
**定义一（语言模型）**：一个语言模型包含一个有限集合$\mathcal{V}$和一个函数$p(x_2,x_2,\dots x_n)$且：

 1. 对于任意$(x_1,x_2 \dots x_n) \in \mathcal{V}^{\dagger}, p(x_1,x_2 \dots x_n) \geq 0$
 2. 且 $\sum_{(x_1 x_2 \dots x_n) \in \mathcal{V}^{\dagger}} p(x_1, x_2, \dots x_n ) = 1$
 
因此$p(x_1, x_2 \dots x_n)$是在集合$\mathcal{V}^{\dagger}$上所有句子的概率分布函数。下面举一个简单的例子来说明从一个训练语料集上学习一个语言模型的方法。令$c(x_1x_2 \dots x_n)$为句子$x_1x_2 \dots x_n$在训练语料集中出现的次数，$N$为训练语料集中句子的总数。那么令$$p(x_1,x_2, \dots x_n) = \frac{c(x_1x_2 \dots x_n)}{N}$$ 这样我们就获得了在该训练集上的一个语言模型。但是这是一个非常差的模型，尤其是当一个未在训练语料集中出现的句子的概率将会被赋值为0。因此这个语言模型无法推广到在训练语料集中未出现的句子。本章也主要介绍一些方法将语言模型推广到在训练语料集上未出现的句子。

乍一看语言建模问题是一个很奇怪的任务，但是我们为什么要考虑这个问题呢？这里有许多原因：

 1. 语言模型在很多应用问题中都非常有用，最常见的是在语音识别和机器翻译中的应用。在许多应用中，对于一个句子是否可能出现在一个语言中的先验概率是十分有用的。例如在语音识别应用中使用语言模型结合声学模型对不同单词的发音进行建模：一方面通过声学模型可以产生一系列的候选句子，同时也包含产生该句子的概率值；语言模型可以基于这些候选句子在该语言中出现的概率对其进行重新排序。
 2. 本章介绍的定义概率函数$p$的算法和估计语言模型的参数的方法可以在其他应用中使用。例如：隐马尔科夫模型及自然语言的解析应用中。

## **1.2 马尔科夫模型**
现在我们来解决一个关键问题：给定一个训练语料集，如何学习概率分布函数$p$？在本节我们介绍马尔科夫模型，在下一节我们介绍tragram语言模型——基于马尔科夫模型的一类重要的语言模型。

### **1.2.1 定长序列的马尔科夫模型**

考虑一个随机变量序列$X_1, X_2, \dots X_n$, 每一个随机变量可以为有限集合$\mathcal{V}$中的任意一个值。现在我们假设序列的长度$n$为一个固定的值（例如 n=100）。在下一节我们会介绍怎么将序列长度$n$推广为一个随机变量，即不同的序列可以有不同的长度。

我们的目标是对任意序列$x_1, x_2, \dots x_n, n \geq 1, x_i \in \mathcal{V}, i \in \{1, \dots, (n-1)\}$的概率进行建模，也就是对如下式的联合概率进行建模：$$P(X_1=x_1, X_2=x_2, \dots X_n=x_n)$$ 对于序列$x_1, x_2, \dots x_n$， 总共有$\left| \mathcal{V} \right|^n$个可能的取值，很显然针对一些$\left| \mathcal{V} \right|$和$n$我们无法罗列出$\left| \mathcal{V} \right|^n$种组合。我们希望建立一个更简洁的模型。

在一阶马尔科夫模型中，我们作如下的假设来简化模型：
$$P(X_1=x_1, X_2=x_2, \dots X_n=x_n) \\ = P(X_1=x_1) \prod_{i=2}^n P(X_i = x_i|X_1=x_1, \dots, X_{i-1}=x_{i-1}) \\ =P(X_1=x_1) \prod_{i=2}^n P(X_i=x_i|X_{i-1}=x_{i-1})$$
在第一步中根据概率的链式法则将任何分布$P(X_1=x_1, X_2=x_2, \dots X_n=x_n)$改写为式？。所以在这一步推到中我们没有做任何假设。但是在第二步中我们做了如下假设：对于任意的$i \in \{2 \dots n\}$和任意序列$x_1,x_2, \dots x_i$有，$$P(X_i=x_i|X_1=x_1, X_2=x_2,\dots, X_n=x_n)=P(X_i=x_i|X_{i-1}=x_{i-1})$$
这就是一阶马尔科夫假设（*first-order Markov  assumption*）。我们假设句子中第$i$个单词的出现仅依赖与它的前一个词即$x_{i-1}$。更正式的讲， 我们假设当给定随机变$X_{i-1}$的情况下，随机变量$X_i$与随机变量$X_1 \dots X_{i-2}$是条件独立的。

在二阶马尔科夫过程中我们对这一假设进行弱化，假设每一个词仅仅依赖于序列中出现在它之前的两个词：$$P(X_i=x_i|X_1=x_1,X_2=x_2, \dots X_n=x_n) = P(X_i|X_{i-2}=x_{i-2}, X_{i-1}=x{i-1})$$ 二阶马尔科夫过程也是trigram语言模型的基本假设。在二阶马尔科夫过程下，整个序列的联合概率公式如下：$$P(X_1=x_1, X_2=x_2, \dots , X_n=x_n) = \prod_{i=2}^{n}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})$$为了方便起见，我们假设$x_0=x_{-1}=*$, 其中$*$是序列中一个特殊的‘start’符号。

### **1.2.2 变长序列的马尔科夫模型**

在前一节中我们假设序列的长度$n$为一个固定值，但是在许多应用中长度$n$是变化的。因此$n$是一个随机变量。针对序列长度的可变性有很多的建模方法，在本节我们为语言模型介绍一种最普通的方法。

我们假设在词序列的第$n$个词（即$X_n$）总是等于一个特殊的符号——STOP符号，并且这个符号只能出现在序列的末尾。例如在二阶马尔科夫假设下我们有，$$P(X_1=x_1, X_2=x_2, \dots , X_n=x_n) = \prod_{i=2}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})$$其中$n \geq 1, x_i \in \mathcal{V}, i \in \{1 \dots (n-1)\}, x_n=STOP$。

因为我们假设语言模型为一个二阶马尔科夫过程，那么将从如下分布中产生每一个词$x_i$:
$$P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})$$
其中$x_i \in \mathcal{V}$, 或者 $x_i=STOP$。如果我们产生了STOP符号，那么序列就生成完毕。反之，我们继续生成序列的下一个词。

我们将生成句子的过程可以写成如下的流程：

 1. 初始化$i=1, x_0 = x_{-1} = *$
 2. 从如下分布中产生词$x_i$ $$P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})$$
 3. 如果$x_i = STOP$, 那么返回序列$x_1,x_2, \dots x_n$。反之，$i = i+1$ 跳转至步骤2。

综上我们获取了一个可以产生变长序列的语言模型。

## **1.3 Trigram语言模型**
语言模型的建模方法有很多种，但是在本章中我们仅关注一个非常重要的语言模型建模方法——trigram语言模型。如上一小节所述，trigram语言模型是马尔科夫模型的直接应用。在本节我们将讨论trigram语言模型的基本定义，采用最大似然估计方法对trigram语言模型的参数进行估计和trigram语言模型的优缺点。

### **1.3.1 基本定义**
与马尔科夫模型一样，我们用$n$个随机变量$X_1,X_2, \dots X_n$对每个句子进行建模。句子的长度$n$也是一个随机变量（不同句子的长度可以不相等），同样的$X_n=STOP$。在二阶马尔科夫模型下，任意句子$x_1,x_2, \dots x_n$的概率可以表示为：
$$P(X_1 = x_1, X_2 = x_2, \dots X_n = x_n ) = \prod_{i=2}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})$$
其中我们假设$x_0=x_{-1}=*$。
我们假设对于任意的i和$x_{i-2},x_{i-1},x_i$有：
$$P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1}) = q(x_i|x_{i-2}, x_{i-1})$$
其中$q(w|u,v)$是语言模型的参数。下面我们将介绍如何在训练语料集上推导及估计模型的参数。有上面公式，语言模型可以改写为如下形式：
$$p(x_1, x_2, \dots x_n) = \prod_{i=1}^n q(x_i|x_{i-2}, x_{i-1})$$
其中$x_1,x_2, \dots x_n$为任意序列。综上trigram语言模型的定义如下。

**定义二（Trigram语言模型）** 一个trigram语言模型包含一个有限集$\mathcal{V}$和参数$q(w|u,c)$，对于任意一个trigram $u, v, w$ 其中$w \in \mathcal{V}\cup\{STOP\}, u,v \in \mathcal{V}\cup\{*\}$。$q(w|u,v)$的值可以解释为：在bigram $(u,v)$ 之后看到此$w$出现的概率。对于任意的句子$x_1,x_2,\dots x_n$, 其中$x_i \in \mathcal{V}, i\in \{1 \dots (n-1)\}, x_n = STOP$, 句子在trigram语言模型下出现的概率为：
$$p(x_1, x_2, \dots x_n) = \prod_{i=1}^n q(x_i|x_{i-2}, x_{i-1})$$
其中$x_0 = x_{-1} = *$。

例如对于句子

> the dog barks STOP

我们有$$p(the dog barks STOP) \\
= q(the|*,*) \times q(dog|*,the) \times q(barks|the,dog) \times q(STOP|dog, barks)$$
需要说明的是在这个表达式中每一个词项就是句子中的一个单词(the, dog, barks, STOP)。在trigram语言模型的假设下，每个单词依赖于出现在它之前的前两个单词。

对于任意的trigram $u,v,w$, 模型的参数满足如下约束：$q(w|u,v) \geq 0$。同时对于任意的bigram $u,v$有$\sum_{w \in \mathcal{V}\cup\{STOP\}}q(w|u,v)$。因此$q(w|u,v)$定义了一个任意词$w$在其上下文$u,v$下的条件概率分布。

接下来我们要解决的是如何估计模型的参数即$q(w|u,v)$其中$w\in \mathcal{V}\cup\{STOP\},  u,v \in \mathcal{V}\cup\{*\}$。显然模型的参数共有$|\mathcal{V}|^3$个，这看起来是一个非常大的数。例如$|\mathcal{V}|=1000$，那么模型就有$|\mathcal{V}|^3 \approx 10^{12}$个参数。下面我们介绍 使用最大似然估计算法对模型的参数进行估计。

### **1.3.2 最大似然估计**
我们首先用最常用的参数估计方法最大似然估计方法对trigram语言模型的参数进行估计。我们将会发现这些估计的参数会有一些瑕疵，但是我们会发现这些方法与实际使用中的参数估计方法有紧密的相关性。

首先我们有如下的定义。令$c(u,v,w)$为trigram$(u,v,w)$在训练语料集中出现的次数。例如c(the, dog, barks)是序列 the dog barks 在训练语料集中出现的次数。同样的，令$c(u,v)$为bigram$(u,v)$在语料集中出现的次数。对于任意的$u,v,w$我们定义：
$$q(w|u,v) = \frac{c(u,v,w)}{c(u,v)}$$
例如我们对参数q(barks|the,dog)的估计就可以写为：
$$q(barks|the,dog) = \frac{c(the, dog, barks)}{c(the, dog)}$$
这个估计是非常自然的：分子是整个trigram （the dog barks)出现的次数，分母是bigram (the dog)出现的次数。我们就简单的将这两项的比率作为参数$q(barks|the, dog)$的估计值。

但是不幸的是，这种参数估计  方法会产生很严重的问题。回想在1.3.1节中我们提到的模型中有太多的参数（一个词汇量为1000的语言模型大概有$10^{12}$个参数需要估计）。因此在训练语料集中许多bigram和trigram出现的次数均为0次，这将产生如下两个问题：

 1. 因为分子$c(u,v,w)$为0， 所以用（1-5）式估计许多词项的值会为0，即$q(w|u,v)=0$。这会造成许多trigram出现的概率被系统性的低估，同时直接将未出现在训练语料集中的trigram的概率赋值为0是很不合理的，因为模型参数的个数远远大于训练语料集中的词项数。
 2. 当分母$c(u,v)=0$时，$q(w|u,v)$是无意义的。

接下来我们将修正上述估计来解决这些问题。但是首先我们讨论下如何评价一个语言模型，然后讨论trigram语言模型的优缺点。

### **1.3.3 语言模型的评价：困惑度(Perplexity)**

那么我们如何评价一个语言模型的特性（表达能力）呢？一种最常用的方法就是计算语言模型在一些held-out数据集上的困惑度(Perplexity)。

评价方法的步骤如下：假设我们有一些测试的句子$x^{(1)}, x^{(2)}, \dots x^{(m)}$。任意一个句子$x^{(i)},i\in\{1 \dots m\}$是词项集合中词汇的一个序列$x_1^{(i)},x_2^{(i)} \dots x_{n_i}^{(i)}$， 其中$n_i$是第i个句子的长度。与之前的假设相同，每一个句子都是以STOP符号结束。

对于测试句子是held-out的是非常重要的， held-out句子集合是指这些测试句子不是训练集中用来估计模型参数的句子，也就是说这些测试句子不属训练语料集和验证语料集，是新的句子。

在给定语言模型下，我们可以计算任意一个测试句子$x^{(i)}$的概率$p(x^{(i)})$。一个最自然的度量语言模型表达能力的指标就是计算其在整个测试集上句子的概率的乘积，也就是：$$\prod_{i=1}^mp(x^{(i)})$$
直观的看上式的值越大，该语言模型对未出现的句子的建模就更好。

在测试集上的困惑度就是在上式的推导变形。令$M$为测试语料集上单词的总数。更准确地，令$n_i$是第$i$个句子的长度，那么有：$$M = \sum_{i=1}^m n_i$$那么语言模型的平均对数概率为$$\frac{1}{M}log_2\prod_{i=1}^m p(x^{(i)}) = \frac{1}{M}\sum_{i=1}^m log_2 p(x_{(i)})$$
即整个测试语料集上所有句子的对数概率的和除以测试语料集上的词项总数。同样的该表达式的值越大，语言模型的表达能力更强。

**困惑度**定义为：$$2^{-\ell}$$ 其中 $$\ell = \frac{1}{M} \sum_{i=1}^m log_2 p(x_{(i)})$$
困惑度是一个正数，由上式可知困惑度越小，语言模型对未出现的句子的建模就更好。

直观的看困惑度的计算表达式，我们可以发现：若我们有词汇集合$\mathcal{V}$，同时$|\mathcal{V}\cup \{STOTP\}| = N$，且该语言模型预测任意trigram $u,v,w$的条件概率为$q(w|u,v) = \frac{1}{N}$。显然这是一个很糟糕的模型，它的$q$函数是一个平均分布，这时改语言模型的困惑度为$N$。所以在平均概率模型下，改语言模型的困惑度就等于词汇量的大小。困惑度可以看作是在特定语言模型下词汇集上有效词汇的数量。例如某个语言模型的困惑度为120（虽然总的词汇量是10000），这就表示在这个语言模型下有120个有效词汇。

更进一步来说，困惑度也等价于$$\frac{1}{\tau}$$ 其中 $$\tau = \sqrt[M]{\prod_{i=1}^m p(x_{(i)})}$$ 即$\tau^M = \prod_{i=1}^m p(x_{(i)})$，又$$\prod_{i=1}^m p(x_{(i)}) = \prod_{i=1}^m \prod_{j=1}^{n_i} q(x_j^{(i)}|x_{j-2}^{(i)},x_{j-1}^{(i)})$$
同时$M=\sum_{i=1}^m n_i$， 那么$\tau$就等于$q(x_j^{(i)}|x_{j-2}^{(i)},x_{j-1}^{(i)})$的几何平均值。例如模型的困惑度为100，那么$\tau = 0.01$，表明其在测试集上的几何平均值为0.01。

下面我们介绍一个关于困惑度的很有意思的现象。如果在测试数据中任意一个trigram $u,v,w$，其条件概率为$q(w|u,v)$，那么该模型在当前测试集上的困惑度就为$\infty$，同时在这种情况下测试集在该模型下的概率值为0，测试集的平均对数概率值为$-\infty$。因此如果我们使用困惑度来评价语言模型的性能，就应该避免对任意trigram的概率估计为0。

最后我们直观地比较下一些困惑度的典型取值。Goodman在文章“A bit of progress in language modeling”中比较了unigram, bigram, trigram三个语言模型在词汇量为5000的数据集上的困惑度。bigram语言模型的参数为$q(w|v)$，同时$$p(x_1,x_2 \dots x_n) = \prod_{i=1}^n q(x_i|x_{i-1})$$因此每个词仅依赖于出现在它之前的词。unigram语言模型的参数为$q(w)$， 同时$$p(x_1,x_2 \dots x_n) = \prod_{i=1}^n q(x_i)$$假设句子中每个词的出现是相互独立的。Goodman的实验结果为：trigram语言模型的困惑度为74左右，bigram语言模型的困惑度为137，unigram语言模型的困惑度为995。对于每一个单词的概率都赋值为1/50000的模型的困惑度为50000。可以看出trigram语言模型的相对于bigram和unigram的性能有很大的提高，相对于最naive的模型则是由显著的提高。

### **1.3.4 Trigram 语言模型的优缺点**

Trigram语言模型的假设要求过于勉强和“不谙语言学的”——每个词的出现仅依赖于在它之前出现的两个词。但是在实际应用中trigram语言模型却有较好的效果。

## **1.4 Trigram模型的平滑估计**

正如前面我们所讨论的，trigram语言模型有大量的参数需要估计。当数据集比较稀疏时，我们采用最大似然估计参数,即$q(w|u,v)=\frac{c(u,v,w)}{c(u,v)}$会遇到一些很严重的问题。即便是使用一个很大的训练集对模型进行训练，$c(u,v,w)$和$c(u,v)$的值很多值都很小或者为零。

本节我们将介绍一些参数估计的平滑方法来解决数据稀疏性带来的很多问题。其最主要的想法还是利用低阶统计量来对参数进行估计，特别地，使用bigram或者unigram的值来平滑trigram语言模型的参数估计。我们将讨论实际应用中常用的两种参数平滑方法：线性插值法(linear interpolation)和折损法(discounting methods)。

### **1.4.1 线性插值法**

一个线性插值的trigram语言模型参数估计方法推导如下。我们定义对trigram,bigram,unigram的参数最大似然估计如下：$$q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}$$ $$q_{ML}(w|v)=\frac{c(v,w)}{c(v)}$$
$$q_{ML}(w)=\frac{c(w)}{c()}$$其中$c(w)$是词$w$在训练语料集中出现的次数，$c()$是训练语料集上词的总数。

trigram, bigram, unigram参数估计各有优缺点。unigram参数估计中的分子和分母永远不会为0:，因此参数的估计总是well-defined，而且参数值永远大于0。但是ungram语言模型完全忽略了词出现的上下文信息，模型也就丢失了很多信息。与此相反，trigram模型确实考虑了词出现的上下文信息，但是其中有很多trigram出现的次数为0。bigram恰好处在这两个极端中间，是unigram和trigram的折中。

线性插值平滑方法是同时使用这三个估计量，我们定义trigram估计量如下：$$q(w|u,v)=\lambda_1 \times q_{ML}(w|u,v) + \lambda_2 \times q_{ML}(w|v) + \lambda_3 \times q_{ML}(w)$$其中$\lambda_1 \lambda_2 \lambda_3$是模型的三个附加参数，它们满足以下条件：$$\lambda_1 \geq 0, \lambda_2 \geq 0, \lambda_3 \geq 0 \\
\lambda_1 + \lambda_2 + \lambda_3 = 1$$因此我们是对这三种参数估计量进行了加权平均。

估计参数$\lambda$的方法有很多，其中最常用的方法如下：我们有一个附加的held-out数据集，该数据集与训练语料集和测试语料集均不同。我们将这个附加的数据集称为development数据集。令$c(u,v,w)$为trigram $(u,v,w)$在development数据集中出现的次数。那么关于$\lambda_1, \lambda_2, \lambda_3$在development数据集上的对数似然函数可以写为：$$L(\lambda_1, \lambda_2, \lambda_3) = \sum_{u,v,w}c(u,v,w)logq(w|u,v) \\
= \sum_{u,v,w}c(u,v,w)log\lambda_2 \times q_{ML}(w|u,v) + \lambda_2 \times q_{ML}(w|v) + \lambda_3 \times q_{ML}(w)$$我们希望选取的参数$\lambda$能够让对数似然函数$L(\lambda_1, \lambda_2, \lambda_3)$的值越大越好。

因此估计参数$\lambda$的值的问题就转化为如下最优化问题：$$argmax_{\lambda_1, \lambda_2, \lambda_3}L(\lambda_1, \lambda_2, \lambda_3) \\
St. \lambda_1 \geq 0, \lambda_2 \geq 0, \lambda_3 \geq 0 \\
\lambda_1 +\lambda_2 + \lambda_3  = 1$$对于上式的优化求解可以获取$\lambda_1, \lambda_2, \lambda_3$的最优值。

如上所述，这种平滑方法包含3个平滑参数$\lambda_1, \lambda_2, \lambda_3$。这三个参数可以解释为trigram, bigram, unigram三种统计量的置信度或者权重。参数$\lambda_1, \lambda_2, \lambda_3$的取值不同体现了参数平滑中不同统计量所占的权值不同，也体现语言模型对不同特性的侧重不同。

在实际应用中，$\lambda_1, \lambda_2, \lambda_3$的变化常常依赖于bigram $u,v$。特别地，当$c(u,v)$较大时，可以加大$\lambda_1$的值——直观的看，当$c(u,v)$较大时我们应该更相信trigram的估计值。

还应该注意的是，当$c(u,v) = 0$时，$\lambda_1$应该为0。这是因为此时trigram的估计量$q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}$
是无定义的。同样的，当$c(u,v)$和$c(v)$为0时，$\lambda_1$和$\lambda_2$的值也应该为零。

在1.5节我们会介绍上述方法的一个拓展方法，叫做bucketing。相对于上述两种方法，有一种简答的参数$\lambda$的估计方法：$$\lambda_1 = \frac{c(u,v)}{c(u,v) + \gamma} \\
\lambda_2 = (1 - \lambda_1) \times \frac{c(v)}{c(v) + \gamma} \\ 
\lambda_3 = 1 - \lambda_1 - \lambda_2 \\ 
St. \lambda_1 \geq 0, \lambda_2 \geq 0, \lambda_3 \geq 0 \\ 
\lambda_1 + \lambda_2 + \lambda_3 = 1$$其中$\gamma > 0$， 是该方法的唯一参数。

在此方法中，我们可以看出$\lambda_1$的变化随$c(u,v)$的变化而变化。同样地，$\lambda_2$也随着$c(v)$的值变化而变化。当$c(u,v) = 0$时，$\lambda_1$为0。当$c(v)=0$时，$\lambda_2$也为0。参数$\gamma$的值同样可以通过在development数据集上的对数似然函数最优化来估计。

总之这种方法相对来说比较粗糙，看起来也不是最优的方法。但是该方法比较简单，在一些实际应用中确实十分有效。

### **1.4.2 折损法(Discounting Methods)**

接下来我们介绍另外一种常用的参数平滑估计方法。首先我们考虑对bigram语言模型的参数进行估计，即我们的目标是确定参数:$$q(w|v)$$其中$w \in \mathcal{V} \cup \{STOP\}, v \in \mathcal{V} \cup \{*\}$。

第一步我们定义一个**折损基数(discounted counts)**.对于任意一个bigram $v,w$的计数$c(v,w) > 0$, 其折损计数定义为:$$c^*(v,w) = c(v,w) - \beta$$其中$\beta$的值大于0小于1(一个常用的取值是$\beta = 0.5$)。因此我们只是从bigram出现的次数中减去一个常数$\beta$。直观的，如果我们使用训练语料集中bigram出现的次数对语言模型参数进行估计，那么我们将系统性的高估bigram在整个语料（该语言实际使用的大规模语料中）中出现的概率，同时对于未在训练集中出现的bigram的概率则估计过低。

对于任意的bigram $(v,w)$ 且出现的频数$c(v,w) > 0$， 其参数估计定义如下：$$q(w|v) = \frac{c^*(v,w)}{c(v)}$$从公式中可以看出我们仅仅用折损计数替换了bigram的原始频数。

对于任意的上下文$v$， 根据上述定义可以诱导出一些缺失概率质量(missing probability mass),即：$$\alpha(v) = 1 - \sum_{w:c(v,w)>0} \frac{c^*(v,w)}{c(v)}$$

|   $x$    |   $c(x)$   |   $c^*(x)$  | $\frac{c^*(x)}{c(the)}$|
|-------        |-------|-------|-------|
|the            |48     |       |       |
|the,dog        |15     |14.5   |14.5/48|
|the,woman      |11     |10.5   |10.5/48|
|the,man        |10     |9.5    |9.5/48 |
|the,park       |5      |4.5    |4.5/48 |
|the,job        |2      |1.5    |1.5/48 |
|the,telescope  |1      |0.5    |0.5/48 |
|the,manual     |1      |0.5    |0.5/48 |
|the,afternoon  |1      |0.5    |0.5/48 |
|the,country    |1      |0.5    |0.5/48 |
|the,street     |1      |0.5    |0.5/48 |

如上表所示例子，表中列出了所有当上下文为the的bigram，即：$v = the, c(the,w)$ 且 $c(v,w) > 0 $。在这个例子中$\beta = 0.5$，所以我们可以计算:$$\sum_{w:c(v,w) > 0} \frac{c^*(v,w)}{c(v)}=\frac{14.5}{48} + \frac{10.5}{48} + \frac{9.5}{48} + \frac{1.5}{48} + \frac{4.5}{48} + 5 \times \frac{0.5}{48} = \frac{43}{48}$$
那么缺失概率质量为：$$\alpha(the) = 1 - \frac{43}{48} = \frac{5}{48}$$
直观的想，我们可以将缺失概率质量分布到那些$c(the,w) = 0$的值上。
更具体地，基于折损计数的平滑估计定义如下：对于任意$v$我们定义集合
$$\mathcal{A}(v) = \{w:c(v,w) > 0\} \\
\mathcal{B}(v) = \{w:c(v,w) = 0\}$$
对于表中的例子来说：$$\mathcal{A}(the) = \{dog, man, woman, park, job, telescope, manual, afternoon, country, street\}$$
$\mathcal{B}(the)$为词汇中除去$\mathcal{A}(the)$的所有词。
那么对参数的估计定义如下：当$w \in \mathcal{A}(v)$时，$q_D(w|v) = \frac{c^*(v,w)}{c(v)}$;当$w \in \mathcal{B}(v)$时，$q_D{w|v} = \alpha(v) \times \frac{q_{ML}(w)}{\sum_{w \in \mathcal{B}(v)}q_{ML}(w)}$因此当$c(v,w) > 0 $时， 估计值为$\frac{c^*(v,w)}{c(v)}$；反之将$\alpha(v)$按照unigram的估计值$q_{ML}(w)$成比例的分配到$w \in \mathcal{B}(v)$中的词。

我们可以将这个方法很容易地推广到trigram语言模型的参数平滑估计上。对于任意的bigram $(u,v)$有如下定义：
$$\mathcal{A}(u,v) = \{w:c(u,v,w) > 0\} \\ 
\mathcal{B}(u,v) = \{w:c(u,v,w) = 0\}$$
对于任意一个trigram $(u,v,w)$， 定义其折损计数为：
$$c^*(u,v,w) = c(u,v,w) - \beta$$
那么trigram语言模型的参数估计如下：若$w \in \mathcal{v}$， $q_D(w|u,v) = \frac{c^*(u,v,w)}{c(u,v)}$；若$w \in \mathcal{B}(v)$， 若$w \in \mathcal{B}(v)$， $q_D(w|u,v) = \alpha(u,v) \times \frac{q_D(w|v)}{\sum_{w \in \mathcal{B}(u,v)q_D(w|v)}}$同样地，缺失概率质量为：
$$\alpha(v) = 1 - \sum_{w \in \mathcal{A}(v)}\frac{c^*(u,v,w)}{c(u,v)}$$与bigram语言模型的参数估计方法一样，trigram语言模型的参数平滑估计方法中仅有一个参数需要估计，即$\beta$。同样的，最优$\beta$也是通过development数据集来选取。